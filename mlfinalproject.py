# -*- coding: utf-8 -*-
"""MLFinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y2BzQi9V2HUYLzTCXJ4eB7RV06Oh2WmX
"""

def read_raw(file_name):
  with open(file_name, 'r') as file:
    data = file.read()
    data=data.split('.')
    for i in range(len(data)):
      data[i]=" ".join(data[i].split())
  return data

def preprocess_raw(text):
  text = text.replace('\u202f', ' ').replace('\xa0', ' ')
  out = ''
  for i, char in enumerate(text.lower()):
    if char.isalpha()==True or char==' ':
      out += char
  out=" ".join(out.split())
  out += ' .'
  return out

def clean_data(file_name):
  raw_text=read_raw(file_name)
  for i in range(len(raw_text)):
    raw_text[i]=preprocess_raw(raw_text[i]).split(" ")
  return raw_text

import os
def build_data(direc):
  data=[]
  counter=0
  for filename in os.listdir(direc):
    counter += 1
    print(counter)
    data += clean_data('./drive/My Drive/pos/'+str(filename))
  return data

len(text)

import collections
import numpy as np
import matplotlib.pyplot as plt
def len_dis(text):
  lens = [len(line) for line in text]
  len_counter = collections.Counter(lens)

  lens = np.array(list(len_counter.keys()))
  sort_idx = np.argsort(lens)
  lens_sort = lens[sort_idx]
  len_counts = np.array(list(len_counter.values()))
  len_counts_sort = len_counts[sort_idx]
  p = np.cumsum(len_counts_sort) / len_counts_sort.sum()
  return p, lens_sort
  
src_p, src_lens_sort = len_dis(text)
plt.plot(src_lens_sort, src_p, 'r-', label='eng')
plt.title('Cumulative Distribution of Sentence Length')
plt.legend()
plt.show()

import collections
import numpy as np
import matplotlib.pyplot as plt
def len_dis(text):
  words = [token for line in text for token in line]
  word_counter = collections.Counter(words)
  freq_counter=  collections.Counter(word_counter.values())
  freqs = np.array(list(freq_counter.keys()))
  freq_length = np.array(list(freq_counter.values()))
  sort_idx= np.argsort(freqs)
  freq_sort = freqs[sort_idx]
  freq_length = np.array(list(freq_counter.values()))
  freq_length_sort = freq_length[sort_idx]
  p = np.cumsum(freq_length_sort) / freq_length_sort.sum()
  return p, freq_sort
  
src_p, src_lens_sort = len_dis(text)
plt.plot(src_lens_sort, src_p, 'r-', label='eng')
plt.title('Cumulative Distribution of Word Occurence')
plt.legend()
axes = plt.gca()
axes.set_xlim([1,100])
plt.show()

class Vocab():
  def __init__(self, name, tokens, min_freq):
    self.name = name
    self.index2word = {
      0: 'pad',
      1: 'bos',
      2: 'eos',
      3: 'unk'
    }
    self.word2index = {v: k for k, v in self.index2word.items()}
    self.num_word = 4
    token_freq = collections.Counter(tokens)
    tokens = [token for token in tokens if token_freq[token] >= MIN_FREQ]
    self._build_vocab(tokens)
    
  def _build_vocab(self, tokens):
    for token in tokens:
      if token not in self.word2index:
        self.word2index[token] = self.num_word
        self.index2word[self.num_word] = token
        self.num_word += 1
        
  def __getitem__(self, tokens):
    if not isinstance(tokens, (list, tuple)):
      return self.word2index.get(tokens, self.word2index['unk'])
    else:
      return [self.__getitem__(token) for token in tokens]

def build_vocab(name, tokens, min_freq):
  tokens = [token for line in tokens for token in line]
  return Vocab(name, tokens, min_freq)

def build_vocabs(lang_src,src_text):
  vocab_src = build_vocab(lang_src, src_text, MIN_FREQ)
  return vocab_src

def pad(line, padding_token):
  return line + [padding_token] * (MAX_LEN + 2 - len(line))

def build_tensor(text, lang, is_source):
  lines = [lang[line] for line in text]
  if not is_source:
    lines = [[lang['bos']] + line + [lang['eos']] for line in lines]
  array = torch.tensor([pad(line, lang['pad']) for line in lines])
  valid_len = (array != lang['pad']).sum(1)
  return array, valid_len

def load_data_nmt(batch_size=2):
  lang_eng, lang_fra = build_vocabs('eng', 'fra', source, target)
  src_array, src_valid_len = build_tensor(source, lang_eng, True)
  tgt_array, tgt_valid_len = build_tensor(target, lang_fra, False)
  train_data = torch.utils.data.TensorDataset(
    src_array, src_valid_len, tgt_array, tgt_valid_len)
  train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)
  return lang_eng, lang_fra, train_iter


source, target = prepare_data(raw_text, max_len=MAX_LEN)
vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size=2)
print('Vocabulary size of source language: {}'.format(vocab_eng.num_word))
print('Vocabulary size of target language: {}'.format(vocab_fra.num_word))
print('Total number of sentence pairs: {}'.format(len(source)))